{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Report\n",
    "\n",
    "    \n",
    "|   * * * * * * * * * *   | **Udacity**         | \n",
    "| -------------           |:-------------:      |\n",
    "| **Nanodegree:**         | *Data Analyst*    | \n",
    "| **Student:**            | *Theresa Kocher*    |\n",
    "| **Date:**               | *28th April 2019*|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "There are 3 sources were we get different parts of our data.\n",
    "\n",
    "1. The twitter archive of the twitter account **WeRateDogs**, which can be found here: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv\n",
    "\n",
    "2. The predicted dog breeds of the images of a **WeRateDogs** tweets can be downloaded here:\n",
    "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "3. The twitter API **`tweepy`** which you can use to access the *favorite_count*, and *retweet_count* for every given tweet_id available in the **WeRateDogs** twitter archive (1). To use the twitter API you need to make a twitter developer account first, and add this project as application, to get an access key and token to request twitter data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering\n",
    "To gather the data from the three sources three different methods were used.\n",
    "\n",
    "1. The twitter archive was downloaded manually and was loaded with the pandas function `pd.read_csv()`.\n",
    "2. The predictions were downloaded programatically using the **`requests`** library. \n",
    "3. The retweet and favourite counts were gathered with the **`tweepy`** twitter API. To avoid automatically blocakge of the API when accessing too much data, you need to use the `wait_on_rate_limit=Tru` and `wait_on_rate_limit_notify=True`parameters in the `tweepy.API()` function. Also you need to use `try except` to avoid that deleted tweets will stop your script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing\n",
    "To assess the data, the following aspects were considered.\n",
    "\n",
    "- The data types were viewed by using the pandas `pd.DataFrame.info()` function.\n",
    "\n",
    "- The number of unique values of colum `twitter_id` were observed with pandas `pd.Series.nunique()`.\n",
    "\n",
    "- Duplicates in the `twitter_id` column were checked with pandas `pd.Series.duplicated()`. (Same result as nunique)\n",
    "\n",
    "- All columns were checked for missings.\n",
    "\n",
    "- Check columns and text to find out how retweets can be found.\n",
    "\n",
    "- Check the validity of the rates by viewing `denominator` values.\n",
    "\n",
    "\n",
    "\n",
    "**Result:**\n",
    "\n",
    "- The columns `timestamp` of the first dataframe and `created_at` of the last dataframe are not of type datetime.\n",
    "\n",
    "- The columns `doggo`, `floofer`, `pupper`, `puppo` are used like type *boolian* but are from *string* data type.\n",
    "\n",
    "- The column `twitter_id` doesn't have duplicates in the first dataframe.\n",
    "\n",
    "- There are rows with missing names in the first dataframe.\n",
    "\n",
    "- Retweets are tweets without image url (`expanded_url` in first dataframe).\n",
    "\n",
    "- Retweets are tweets with a retweet reference (`retweeted_status_id` in first dataframe).\n",
    "\n",
    "- Not all ratings have valid `denominator`. Only 10 is a valid denominator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Taking the results from the assessing step, we can derivate the following cleaning steps:\n",
    "\n",
    "- Convert `timestamp` and `created_at` columns to datetime.\n",
    "\n",
    "- Convert `doggo`, `floofer`, `pupper` and `puppo` columns to bool by replacing the values.\n",
    "\n",
    "- Remove tweets without images (`expanded_url`).\n",
    "\n",
    "- Remove retweets with retweet id (`retweeted_status_id`).\n",
    "\n",
    "- Reomving tweets without dog `name`. This was decided to have a name to every tweets dog later in analysis.\n",
    "\n",
    "- Remove rows where rating_denominator != 10.\n",
    "\n",
    "- Remove when prediction `p1_dog` is `False` in third dataframe.\n",
    "\n",
    "- Remove newer tweets than August 1st, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying\n",
    "The three tables contain information about mostly the same tweets. So it is necessary to bring the information of all three tables togehther into one. This is why we join the tables and only keep those columns, we need for analysis later. Following steps were taken for tidying the data:\n",
    "\n",
    "- Inner join on tweet_id (keep only tweets where data is available in all 3 dataframes).\n",
    "\n",
    "- Take ranking columns from first dataframe(`rating_numerator`, `rating_denominator`).\n",
    "\n",
    "- Take `name` column from first dataframe.\n",
    "\n",
    "- Extract image from second dataframe (`jpg_url`).\n",
    "\n",
    "- Take `retreat_count` from third dataframe.\n",
    "\n",
    "- Take `favorite_count` from third dataframe.\n",
    "\n",
    "- Take all 3 predictions and confidence values from third dataframe.\n",
    "\n",
    "- Remove needless columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
